{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aacf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BrightEarth AI Refactored Notebook\n",
    "\n",
    "# Cell 1: Imports & Secure Setup\n",
    "import os\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "from ibm_watsonx_ai.credentials import Credentials\n",
    "\n",
    "# Load sensitive credentials from environment variables\n",
    "API_KEY = os.getenv(\"WATSONX_API_KEY\",\"ch1nbczDMcqQRSQY5EeWxvGL5R_DfCig43bnKxxfB58w\")\n",
    "PROJECT_ID = os.getenv(\"WATSONX_PROJECT_ID\",\"ae528503-ba43-464b-8eb2-add4c13852ce\")\n",
    "REGION = os.getenv(\"WATSONX_REGION\", \"us-south\")\n",
    "\n",
    "if not API_KEY or not PROJECT_ID:\n",
    "    raise EnvironmentError(\"Please set WATSONX_API_KEY and WATSONX_PROJECT_ID environment variables.\")\n",
    "\n",
    "# Initialize IBM WatsonX Granite model inference\n",
    "creds = Credentials(\n",
    "    url=f\"https://{REGION}.ml.cloud.ibm.com\",\n",
    "    api_key=API_KEY\n",
    ")\n",
    "model = ModelInference(\n",
    "    model_id=\"ibm/granite-3-8b-instruct\",\n",
    "    credentials=creds,\n",
    "    project_id=PROJECT_ID\n",
    ")\n",
    "\n",
    "# Cell 2: Central Inference Utility\n",
    "def run_inference(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 80,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    frequency_penalty: float = 0.0,\n",
    "    presence_penalty: float = 0.0,\n",
    "    stop_sequences: list = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Utility to send prompts to the Granite model and return generated text.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "    }\n",
    "    if stop_sequences:\n",
    "        params[\"stop_sequences\"] = stop_sequences\n",
    "\n",
    "    response = model.generate(prompt=prompt, params=params)\n",
    "    return response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "# Cell 3: Eco-Action Tip Generator\n",
    "INSTRUCTION = (\n",
    "    \"You are BrightEarth AI, a sustainability co-pilot.\\n\"\n",
    "    \"If role, environment, or task is missing, ask for them.\\n\"\n",
    "    \"Once all are provided, return ONE eco-action tip (<40 words), human-sounding, realistic, aligned to UN SDG 8.\\n\"\n",
    "    \"Never mention user info. Never output more than one tip. Do not use titles or citations. Only reply with the tip.\"\n",
    ")\n",
    "\n",
    "def generate_tip(job_role: str, work_env: str, current_task: str) -> str:\n",
    "    if not (job_role and work_env and current_task):\n",
    "        return \"Can you remind me of your job role, work environment, and current task?\"\n",
    "    prompt = (\n",
    "        f\"{INSTRUCTION}\\n\\n\"\n",
    "        f\"Role={job_role}; Environment={work_env}; Task={current_task}\\n\"\n",
    "    )\n",
    "    return run_inference(\n",
    "        prompt,\n",
    "        max_new_tokens=80,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        frequency_penalty=0.2,\n",
    "        presence_penalty=0.2,\n",
    "        stop_sequences=[\"<|endoftext|>\"]\n",
    "    )\n",
    "\n",
    "# Cell 4: Summarizer Function\n",
    "def summarize_text(text: str) -> str:\n",
    "    if not text.strip():\n",
    "        return \"Please provide text to summarize.\"\n",
    "    prompt = f\"Summarize the following text into two clear, professional sentences:\\n\\n{text}\\n\"\n",
    "    return run_inference(prompt, max_new_tokens=60, temperature=0.5)\n",
    "\n",
    "# Cell 5: Professional Report Writer\n",
    "def generate_report(topic: str) -> str:\n",
    "    if not topic.strip():\n",
    "        return \"Please provide a topic for the report.\"\n",
    "    prompt = (\n",
    "        f\"Draft a concise professional report on '{topic}'.\"\n",
    "        \" Include an introduction, key points, and a conclusion.\"\n",
    "    )\n",
    "    return run_inference(prompt, max_new_tokens=1000, temperature=0.7)\n",
    "\n",
    "# Cell 6: Innovation Trend Finder\n",
    "def innovation_trend(job_role: str) -> str:\n",
    "    if not job_role.strip():\n",
    "        return \"Please provide a job role to discover relevant trends.\"\n",
    "    prompt = (\n",
    "        f\"What is one cutting-edge innovation or technology trend that a {job_role} should know about?\"\n",
    "        \" Provide one actionable insight.\"\n",
    "    )\n",
    "    return run_inference(prompt, max_new_tokens=60, temperature=0.7)\n",
    "\n",
    "# Cell 7: Vision Model Setup\n",
    "vision_model = ModelInference(\n",
    "    model_id=\"ibm/granite-vision-2b-vision-instruct\",\n",
    "    credentials=creds,\n",
    "    project_id=PROJECT_ID\n",
    ")\n",
    "\n",
    "# Cell 8: Eco-Action Proof Verifier\n",
    "def verify_proof(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses Granite Vision to verify and describe an eco-action proof image.\n",
    "    Returns a brief description or classification.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        img_bytes = f.read()\n",
    "    prompt = \"Verify this image as valid proof of an eco-action by describing what it shows.\"\n",
    "    response = vision_model.generate(\n",
    "        prompt=prompt,\n",
    "        inputs={\"image\": img_bytes},\n",
    "        params={\"max_new_tokens\": 100, \"temperature\": 0.5}\n",
    "    )\n",
    "    return response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "# Example usage (uncomment and replace path):\n",
    "# print(verify_proof(\"path/to/recycling_photo.png\"))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
